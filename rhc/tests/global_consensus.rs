use rhc::{
    consensus::BftConsensus,
    test_utils::{create_test_cluster, LatencyMeasurement},
    NodeId,
};
use std::sync::Arc;
use tokio::sync::mpsc;

#[tokio::test]
async fn test_bft_consensus_basic_agreement() {
    // Create 4 global arbitrators (can tolerate 1 Byzantine failure)
    let (tx, mut rx) = mpsc::unbounded_channel();
    
    let arbitrators: Vec<Arc<BftConsensus>> = (0..4)
        .map(|_| {
            Arc::new(BftConsensus::new(
                NodeId::new(),
                3, // threshold: 2f+1 = 3
                4, // total nodes
                tx.clone(),
            ))
        })
        .collect();
    
    let test_value = b"global_consensus_value".to_vec();
    let measurement_start = std::time::Instant::now();
    
    // First arbitrator proposes a value
    arbitrators[0].propose(test_value.clone()).await.unwrap();
    
    // Simulate message passing between arbitrators
    let mut message_count = 0;
    let max_messages = 1000; // Prevent infinite loops in test
    
    while message_count < max_messages {
        if let Ok(message) = tokio::time::timeout(
            tokio::time::Duration::from_millis(10),
            rx.recv()
        ).await {
            if let Some(msg) = message {
                message_count += 1;
                
                // Broadcast to all other arbitrators
                match msg {
                    rhc::message::Message::Propose(proposal) => {
                        for arb in &arbitrators {
                            if arb.current_round() <= proposal.round {
                                arb.handle_proposal(proposal.clone()).await.unwrap();
                            }
                        }
                    }
                    rhc::message::Message::ThresholdShare(share) => {
                        for arb in &arbitrators {
                            arb.handle_share(share.clone()).await.unwrap();
                        }
                    }
                    rhc::message::Message::Commit(commit) => {
                        for arb in &arbitrators {
                            arb.handle_commit(commit.clone()).await.unwrap();
                        }
                        
                        // Check if all arbitrators have committed
                        let round = commit.round;
                        let all_committed = arbitrators.iter().all(|a| {
                            a.get_committed_value(round).is_some()
                        });
                        
                        if all_committed {
                            let elapsed = measurement_start.elapsed();
                            println!("BFT consensus completed in: {}ms", elapsed.as_millis());
                            
                            // Should complete within 500ms as per RHC spec
                            assert!(elapsed.as_millis() <= 500, "Global consensus too slow: {}ms", elapsed.as_millis());
                            
                            // Verify all arbitrators agreed on same value
                            for arb in &arbitrators {
                                let committed = arb.get_committed_value(round).unwrap();
                                assert_eq!(committed, test_value);
                            }
                            
                            return;
                        }
                    }
                    _ => {}
                }
            }
        } else {
            break; // Timeout - consensus should have completed by now
        }
    }
    
    panic!("BFT consensus did not complete within expected time");
}

#[tokio::test]
async fn test_bft_with_byzantine_node() {
    let (tx, mut rx) = mpsc::unbounded_channel();
    
    // Create 7 arbitrators (can tolerate 2 Byzantine failures)
    let arbitrators: Vec<Arc<BftConsensus>> = (0..7)
        .map(|_| {
            Arc::new(BftConsensus::new(
                NodeId::new(),
                5, // threshold: 2f+1 = 5
                7, // total nodes
                tx.clone(),
            ))
        })
        .collect();
    
    let correct_value = b"correct_value".to_vec();
    let byzantine_value = b"byzantine_value".to_vec();
    
    // Honest nodes propose correct value
    arbitrators[0].propose(correct_value.clone()).await.unwrap();
    
    // Byzantine nodes try to propose different value
    arbitrators[1].propose(byzantine_value.clone()).await.unwrap();
    arbitrators[2].propose(byzantine_value.clone()).await.unwrap();
    
    let start_time = std::time::Instant::now();
    let mut rounds_seen = std::collections::HashSet::new();
    
    // Process messages with Byzantine behavior simulation
    while start_time.elapsed().as_millis() < 1000 {
        if let Ok(Some(msg)) = tokio::time::timeout(
            tokio::time::Duration::from_millis(10),
            rx.recv()
        ).await {
            match msg {
                rhc::message::Message::Propose(proposal) => {
                    rounds_seen.insert(proposal.round);
                    
                    // Only honest nodes process proposals normally
                    for (i, arb) in arbitrators.iter().enumerate() {
                        if i >= 3 { // Nodes 3-6 are honest
                            arb.handle_proposal(proposal.clone()).await.unwrap();
                        }
                    }
                }
                rhc::message::Message::ThresholdShare(share) => {
                    // Byzantine nodes might send invalid shares, but honest nodes proceed
                    for (i, arb) in arbitrators.iter().enumerate() {
                        if i >= 3 { // Only honest nodes
                            let _ = arb.handle_share(share.clone()).await; // Ignore errors from Byzantine shares
                        }
                    }
                }
                rhc::message::Message::Commit(commit) => {
                    for arb in &arbitrators {
                        arb.handle_commit(commit.clone()).await.unwrap();
                    }
                    
                    // Check if honest majority has committed
                    let honest_committed: Vec<_> = arbitrators.iter().skip(3)
                        .filter_map(|a| a.get_committed_value(commit.round))
                        .collect();
                    
                    if honest_committed.len() >= 4 { // Honest majority
                        // All honest nodes should agree on the same value
                        let first_value = &honest_committed[0];
                        assert!(honest_committed.iter().all(|v| v == first_value));
                        
                        println!("BFT with Byzantine nodes completed, honest nodes agreed on: {:?}", 
                                String::from_utf8_lossy(first_value));
                        
                        return;
                    }
                }
                _ => {}
            }
        }
    }
    
    // Even with Byzantine nodes, honest majority should reach consensus
    panic!("BFT consensus with Byzantine nodes failed to complete");
}

#[tokio::test]
async fn test_threshold_signature_aggregation() {
    use rhc::crypto::{BlsKeyPair, ThresholdSignatureAggregator};
    
    let threshold = 5;
    let num_nodes = 7;
    
    // Generate keypairs
    let keypairs: Vec<BlsKeyPair> = (0..num_nodes)
        .map(|_| BlsKeyPair::generate())
        .collect();
    
    let message = b"test_message_for_aggregation";
    let mut aggregator = ThresholdSignatureAggregator::new(threshold);
    
    // Each node signs the message
    for (i, keypair) in keypairs.iter().enumerate() {
        let signature = keypair.sign(message);
        let node_id = NodeId::new();
        
        aggregator.add_share(node_id, &signature).unwrap();
        
        if i + 1 >= threshold {
            assert!(aggregator.has_threshold());
            
            // Test aggregation
            let measurement = std::time::Instant::now();
            let aggregated = aggregator.aggregate().unwrap();
            let agg_time = measurement.elapsed();
            
            println!("Threshold signature aggregation took: {}Î¼s", agg_time.as_micros());
            
            // Should be very fast (< 1ms)
            assert!(agg_time.as_micros() < 1000, "Signature aggregation too slow");
            
            // Verify the aggregated signature (simplified check)
            assert!(matches!(aggregated, rhc::crypto::Signature::Bls(_)));
            
            break;
        } else {
            assert!(!aggregator.has_threshold());
        }
    }
}

#[tokio::test]
async fn test_global_consensus_under_network_partition() {
    let (nodes, network) = create_test_cluster(0, 0, 6).await; // 6 global arbitrators
    
    for node in &nodes {
        node.start().await.unwrap();
    }
    
    // Create partition: 3 nodes on each side
    let group1: Vec<_> = nodes.iter().take(3).collect();
    let group2: Vec<_> = nodes.iter().skip(3).collect();
    
    // Partition the network
    for n1 in &group1 {
        for n2 in &group2 {
            network.partition(n1.id, n2.id);
        }
    }
    
    // Both sides try to make progress independently
    let _test_value = b"partition_test_value".to_vec();
    
    // Group 1 might be able to make progress if it has >2f+1 nodes
    // But in this case, neither side has majority (3 < 4 needed for f=2)
    
    tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;
    
    // Heal the partition
    for n1 in &group1 {
        for n2 in &group2 {
            network.heal_partition(n1.id, n2.id);
        }
    }
    
    // Now consensus should be able to proceed
    // In a real implementation, nodes would resume consensus after partition heals
    
    println!("Network partition test completed - system survived partition");
}

#[tokio::test] 
async fn test_global_eventual_consistency() {
    let (nodes, network) = create_test_cluster(3, 2, 3).await;
    
    for node in &nodes {
        node.start().await.unwrap();
    }
    
    let local_leaders: Vec<_> = nodes.iter().skip(5).collect(); // Last 3 are local
    
    // Each local leader performs operations
    for (i, leader) in local_leaders.iter().enumerate() {
        let lease = leader.request_lease(&format!("global_domain_{}", i), chrono::Duration::seconds(30))
            .await
            .unwrap();
        
        leader.write(&format!("global_key_{}", i), format!("global_value_{}", i).into_bytes(), lease)
            .await
            .unwrap();
    }
    
    // Wait for global convergence (this would be much longer in reality)
    tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;
    
    // All nodes should eventually have consistent view of global state
    // In practice, this would involve complex state synchronization
    
    println!("Global eventual consistency test completed");
    
    // Verify that all operations were recorded (simplified check)
    for leader in &local_leaders {
        // Check that operations exist in storage
        for i in 0..local_leaders.len() {
            let key = format!("global_key_{}", i);
            let result = leader.storage().get(&key).await.unwrap();
            if i < local_leaders.len() {
                // Operations should exist somewhere in the system
                // (exact convergence rules depend on CRDT implementation)
            }
        }
    }
}

#[tokio::test]
async fn test_compact_proof_size() {
    // Test that BLS signatures provide compact proofs as claimed
    use rhc::crypto::{BlsKeyPair, ThresholdSignatureAggregator};
    
    let num_signers = 100; // Large number of signers
    let threshold = 67; // 2f+1 where f=33
    
    let keypairs: Vec<BlsKeyPair> = (0..num_signers)
        .map(|_| BlsKeyPair::generate())
        .collect();
    
    let message = b"large_scale_consensus_message";
    let mut aggregator = ThresholdSignatureAggregator::new(threshold);
    
    // Collect signatures from threshold number of nodes
    for keypair in keypairs.iter().take(threshold) {
        let signature = keypair.sign(message);
        let node_id = NodeId::new();
        aggregator.add_share(node_id, &signature).unwrap();
    }
    
    // Aggregate
    let aggregated = aggregator.aggregate().unwrap();
    
    // Measure size
    let serialized = bincode::serialize(&aggregated).unwrap();
    let proof_size = serialized.len();
    
    println!("Compact proof size for {} signers: {} bytes", threshold, proof_size);
    
    // Should be much smaller than individual signatures
    // BLS signature should be ~48 bytes regardless of number of signers
    assert!(proof_size < 100, "Proof not compact enough: {} bytes", proof_size);
    
    // Compare to naive approach (individual signatures)
    let individual_size = threshold * 64; // Ed25519 signatures are 64 bytes each
    let compression_ratio = individual_size as f64 / proof_size as f64;
    
    println!("Compression ratio: {:.1}x smaller than individual signatures", compression_ratio);
    assert!(compression_ratio > 10.0, "Insufficient compression: {:.1}x", compression_ratio);
}